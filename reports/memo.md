# Addressing the merits of an inquiry into housing prices…
The U.S. housing market value sits anywhere between $27 trillion and $33.3. Nearly every industry is at least peripherally involved in the market. These features without any analysis to the outside perspective should be obvious but it is important to validate these assumptions.

# Describing the features of our base model…
Our base model examines the cumulative impacts on a residence’s sale price of the following: that given residence’s livable square footage, the square footage of its porch (if indeed it has one), the “nuisances” (i.e., power lines and traffic or airport noise) to which it’s subjected, given the the lot’s situation, and whether or not its property sits on a waterfront.

# Discussing the legitimacy of the results of our base model…
We must note that our base model suffered from an inability to satisfy the four major preconditions required for a responsible regression by ordinary least-squares. Even before examining possible collinearities and multicollinearities, a plot of estimated points to error terms (i.e., residuals) proved not to be normally distributed. That meant that the model failed to satisfy the following requirements: homoscedasticity (normally distributed error terms); independent linearity of predictors in relationship the target variable; and independence of error terms. 

# Describing how we chose to refine our data…
No outliers or highly leveraged observations were omitted; nor were any deletions made of suspectly collected observations. We set a sale price threshold of $50,000 and a ceiling of $2.5MM.

# Noting the tools on which we particularly relied…
Of greatest use to our data prep and analysis were Scikit-Learn’s One-Hot Encoding method (for the transformation of categorical variables into metrical ones via “dummification”) and StatsModel’s OLS package for regressing and providing the basis for our descriptive statistics. 

# Answering why we picked the features we did... 
In devising our “advanced” model, we simply selected features based on the intuitive logic of a given parameter affecting residential sale prices and whether or not it would fit into our model. 

# Detailing how we visualized our results for interpretability...
After building an OS model composed of our chosen features, we created four visualizations - the very same for the regression performed for our base model: A Q-Q graph; a scatterplot of estimates mapped to residuals; and a correlation matrix of the features.

# Discussing the legitimacy of our results...
A visual inspection of our estimate-to-residual plot - along with a confirmatory test of the normality of its distribution - proved that the OLS requirements had failed to be met. Much as we note in discussing the shortcomings of our base model, we must acknowledge here that our “advanced” model fails to prove error term independence,, homoscedasticity, and the linearity of the features to the response variable. Though these requirement failures led us to back-burner an examination of multicollinearities, we did drop one of each pair of features having a correlation coefficient greater than 0.8, as this was easily handled. The feature-to-feature correlation matrix of the final model lends some insight as to what collinearity remains; and at a first pass - and though quite densely populated (and narrowly color-graded) - the matrix seem to suggest a surprisingly welcome featural independence, at least in a pair-wise sense.. Finally, in evaluating our Q-Q plot (as a means of visually representing the relative linearity of the quantiles of our error terms), we unfortunately will encounter once more what we confirmed in our test of normality of the distribution of the error terms: that our errors terms are not normally distributed and, as such, impugn the reliability of our model as a predictive tool of residential pricing within King County. 

# Discussing our confidence in the results of our advanced model...
We enjoy a fairly substantial adjusted R-squared with our model. This is the objective measure of a model’s predictive ability, irrespective of that model’s inferential suitability. As forceful as such a statement is, however, we must remember that, as mentioned above, we failed to satisfy all four major OLS requirements. As such, it would be irresponsible to deploy this model to some actionable ends. 

# Identifying the shortcomings of our “advanced” model...
Since it was easier to disqualify our model on the basis of its failure of the test of the estimate-to-residual distribution’s normality (by which we negated the assumptions of feature-to-response linearity, as well as those of homoscedasticity and the independence of error terms), we did not scrutinize possible featural multicollinearities. (As such, we might want to do a variance inflation factor calculation if continuing with the project.) Further issues may have been introduced via a massive division of the observations of a given feature upon its being One-Hot-Encoding - either by skewing the regression line if a meaningless division’s values were included in the model, or omitting meaningful divisions should that division’s small size resulted in noise sufficient to dissauding that predictor’s inclusion.
